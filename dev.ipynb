{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import googlemaps\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many free credits you get this month...\n",
    "# clean up collect_and_cache data function...\n",
    "# comment the functions...\n",
    "# how to do this with classes?\n",
    "\n",
    "\n",
    "# feature collection from all reviews..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSave jsons of gmap queries for 100 major cities in LA\\nBegin to get a handle on text sentiment analyses\\n\\nIdea:\\nin spatial transcriptomics, we often define \"niches\" to be recurring neighborhoods of gene expression or cell types. \\nIn one version of this, we look at the proportion of cells within a radius around each cell, and cluster these cell type neighborhood compositions. \\n\\nHere, based on the vibes of a bunch of shops, maybe just cafes, or a collection of cafes, bookstores, grocery stores etc, \\ncompute the proportion of each type within a radius around each major city and determine the overall \"vibe\"of the city...\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save jsons of gmap queries for a few major cities in LA\n",
    "Begin to get a handle on text sentiment analyses\n",
    "\n",
    "Idea:\n",
    "in spatial transcriptomics, we often define \"niches\" to be recurring neighborhoods of gene expression or cell types. \n",
    "In one version of this, we look at the proportion of cells within a radius around each cell, and cluster these cell type neighborhood compositions. \n",
    "\n",
    "Here, based on the vibes of a bunch of shops, maybe just cafes, or a collection of cafes, bookstores, grocery stores etc, \n",
    "compute the proportion of each type within a radius around each major city and determine the overall \"vibe\"of the city...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = googlemaps.Client(key='AIzaSyCCw8D8JwwBQkKvbF7yLWCgijKwJpmO6iM')\n",
    "LA_AREAS = {\n",
    "    'Downtown LA': (34.0522, -118.2437),\n",
    "    'Santa Monica': (34.0195, -118.4912),\n",
    "    'Hollywood': (34.0928, -118.3287),\n",
    "    'Beverly Hills': (34.0736, -118.4004),\n",
    "    'Venice': (33.9850, -118.4695)\n",
    "}\n",
    "BUSINESS_TYPES = ['cafe', 'restaurant', 'book_store']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS = 5000\n",
    "CACHE_DIR = './gmap_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "#def get_city_data()\n",
    "#def get_business_details()\n",
    "\n",
    "def collect_and_cache_data(cache_dir: str, areas:  Dict[str, Tuple], business_types: List[str], radius: int = 5000):\n",
    "    if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_cache_file = os.path.join(cache_dir, f\"full_collection_{timestamp}.json\")\n",
    "    \n",
    "    all_data = {\n",
    "        \"collection_info\": {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"areas_searched\": list(areas.keys()),\n",
    "            \"business_types\": business_types,\n",
    "            \"radius\": radius\n",
    "        },\n",
    "        \"areas\": {}\n",
    "    }\n",
    "\n",
    "    for city,location in areas.items():\n",
    "        city_cache_file = os.path.join(cache_dir, f\"{city.replace(' ', '_').lower()}.json\")\n",
    "        if os.path.exists(city_cache_file):\n",
    "            print(f\"  Loading cached data for {city}\")\n",
    "            with open(city_cache_file, 'r', encoding='utf-8') as f:\n",
    "                city_data = json.load(f)\n",
    "            all_data[\"areas\"][city] = city_data\n",
    "            continue\n",
    "\n",
    "        lat,lng = location\n",
    "        city_data = {\n",
    "            \"city_info\": {\n",
    "                \"name\": city,\n",
    "                \"coordinates\": {\"lat\": lat, \"lng\": lng},\n",
    "                \"search_radius\": radius,\n",
    "                \"fetch_timestamp\": datetime.now().isoformat()\n",
    "            },\n",
    "            \"businesses\": {}\n",
    "        }\n",
    "\n",
    "        for business_type in BUSINESS_TYPES:\n",
    "            places_result = gmaps.places_nearby(location=(lat, lng), radius=radius, type=business_type)\n",
    "            places = places_result.get('results',[])    \n",
    "\n",
    "            business_details = []\n",
    "            for place in places:\n",
    "                place_id = place['place_id']\n",
    "                details = gmaps.place(\n",
    "                    place_id=place_id,\n",
    "                    fields=['name', 'rating', 'reviews', 'price_level'] #editorial_summary\n",
    "                )\n",
    "                combined_data = {\n",
    "                    \"basic_info\": place,\n",
    "                    \"detailed_info\": details.get('result', {}),\n",
    "                    \"search_type\": business_type\n",
    "                }\n",
    "                business_details.append(combined_data)\n",
    "                time.sleep(0.1)\n",
    "            city_data[\"businesses\"][business_type] = business_details\n",
    "            time.sleep(1)\n",
    "\n",
    "        with open(city_cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(city_data, f, ensure_ascii=False, indent=2)\n",
    "        all_data[\"areas\"][city] = city_data\n",
    "        print(f\"  Cached data for {city} to {city_cache_file}\")\n",
    "\n",
    "    with open(session_cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nComplete dataset saved to: {session_cache_file}\")\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cached data for Downtown LA to ./gmap_cache\\downtown_la.json\n",
      "  Cached data for Santa Monica to ./gmap_cache\\santa_monica.json\n",
      "  Cached data for Hollywood to ./gmap_cache\\hollywood.json\n",
      "  Cached data for Beverly Hills to ./gmap_cache\\beverly_hills.json\n",
      "  Cached data for Venice to ./gmap_cache\\venice.json\n",
      "\n",
      "Complete dataset saved to: ./gmap_cache\\full_collection_20250703_214634.json\n"
     ]
    }
   ],
   "source": [
    "all_data = collect_and_cache_data(CACHE_DIR,LA_AREAS,BUSINESS_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafe-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
